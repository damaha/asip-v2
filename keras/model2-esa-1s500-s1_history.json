{
    "val_loss": [
        0.5327755212783813,
        0.041254233568906784,
        0.37068310379981995,
        0.03322703763842583,
        0.018299875780940056,
        0.08524379134178162,
        0.17597512900829315,
        0.2692740261554718,
        0.07115978747606277,
        0.5500122904777527,
        0.02762422524392605,
        0.15623928606510162,
        0.09627111256122589,
        0.02219937928020954,
        0.02627616375684738,
        0.13570202887058258,
        0.016800852492451668,
        0.2687305510044098,
        0.16320988535881042,
        0.522233784198761,
        0.40745314955711365,
        0.15018461644649506,
        0.021030675619840622,
        0.021063342690467834,
        0.43699291348457336,
        0.3635181188583374,
        0.1928856074810028,
        0.3154529929161072,
        0.23282690346240997,
        0.10338642448186874,
        0.08212006837129593,
        0.046755608171224594,
        0.25325971841812134,
        0.17017032206058502,
        0.26150691509246826,
        0.09355508536100388,
        0.3516278564929962,
        0.2572866380214691,
        0.3376823365688324,
        0.021634012460708618,
        0.661090075969696,
        0.5459078550338745,
        0.3134118616580963,
        0.19150716066360474,
        0.03340192511677742,
        0.11262962222099304,
        0.04241231083869934,
        0.17694054543972015,
        0.01915164664387703,
        0.09521592408418655,
        0.26836326718330383,
        0.015420546755194664,
        0.32540276646614075,
        0.08803753554821014,
        0.08415903896093369,
        0.20283205807209015,
        0.0775262638926506,
        0.042909566313028336,
        0.27266085147857666,
        0.03992181643843651
    ],
    "val_acc": [
        0.7087545394897461,
        0.8628831505775452,
        0.8336268663406372,
        0.8803537487983704,
        0.8351640105247498,
        0.803836464881897,
        0.866295337677002,
        0.8114396333694458,
        0.8774154782295227,
        0.7153599262237549,
        0.8694121837615967,
        0.8482763171195984,
        0.861426830291748,
        0.8837923407554626,
        0.8502082824707031,
        0.6792833209037781,
        0.8447571992874146,
        0.844773530960083,
        0.8648092150688171,
        0.8061447739601135,
        0.7709977030754089,
        0.8582679033279419,
        0.8575199246406555,
        0.8447211980819702,
        0.8306026458740234,
        0.8199493288993835,
        0.8279826045036316,
        0.8465808033943176,
        0.8512108325958252,
        0.8794284462928772,
        0.8462122082710266,
        0.8161191940307617,
        0.8537623286247253,
        0.7975491285324097,
        0.8423818945884705,
        0.8625245690345764,
        0.8452200889587402,
        0.8222959041595459,
        0.8473849296569824,
        0.8479800820350647,
        0.8424170017242432,
        0.8374245762825012,
        0.8510162830352783,
        0.8608918786048889,
        0.832457959651947,
        0.8287301659584045,
        0.854136049747467,
        0.8683872222900391,
        0.8551948070526123,
        0.8588644862174988,
        0.8392850160598755,
        0.850303590297699,
        0.7682622075080872,
        0.8631291389465332,
        0.8582943677902222,
        0.8440768718719482,
        0.8233349323272705,
        0.8365219235420227,
        0.8615040183067322,
        0.7852490544319153
    ],
    "loss": [
        0.28802077437074763,
        0.23200506531935455,
        0.217758229536754,
        0.208537939125678,
        0.20273507253275294,
        0.19266840402495347,
        0.19369182466091323,
        0.19081300328463408,
        0.19261828951524976,
        0.18860360125727405,
        0.1790550566207406,
        0.1768301667312271,
        0.17854374722314117,
        0.19135900813056209,
        0.17166698408807635,
        0.17788407567422837,
        0.16430273401919146,
        0.1641996276170227,
        0.16633236522874156,
        0.17282197011595,
        0.1633797811075939,
        0.16353379312399727,
        0.1506714326872801,
        0.15347909185980044,
        0.14585911794246753,
        0.13845237310330843,
        0.1492418460707356,
        0.14932771463765948,
        0.14471660432940017,
        0.14990078391945255,
        0.14798543493995006,
        0.14869986207781538,
        0.14803270524210063,
        0.1465064382219076,
        0.1372077969994543,
        0.13779532309478384,
        0.15055060660384129,
        0.14694369006035632,
        0.13206460282615043,
        0.15188395450970407,
        0.13972572998570287,
        0.1470688840863776,
        0.14357448430089353,
        0.140924162009234,
        0.13542433911772056,
        0.1476963325776641,
        0.14690328726965515,
        0.1507453547162846,
        0.1386553089119956,
        0.1440315066371414,
        0.15304322770462211,
        0.14271037133220457,
        0.1483844017949273,
        0.1425971486386109,
        0.14247025289959023,
        0.14019089347658226,
        0.14621101683444881,
        0.14899353517892647,
        0.14156219661289957,
        0.13933666726214528
    ],
    "acc": [
        0.8409708142280579,
        0.8428175449371338,
        0.8324983716011047,
        0.8338651657104492,
        0.8308860659599304,
        0.8328053951263428,
        0.8404001593589783,
        0.8383017778396606,
        0.8325722217559814,
        0.8320670127868652,
        0.8417333364486694,
        0.8418686389923096,
        0.8367018699645996,
        0.8342027068138123,
        0.8389468789100647,
        0.8268741965293884,
        0.8372577428817749,
        0.8386570811271667,
        0.8337753415107727,
        0.8417792320251465,
        0.8414040803909302,
        0.8307890295982361,
        0.8393510580062866,
        0.8375732898712158,
        0.8388321399688721,
        0.8465265035629272,
        0.8351629376411438,
        0.8405869603157043,
        0.8348827362060547,
        0.8336992263793945,
        0.8399765491485596,
        0.8446388840675354,
        0.8397102952003479,
        0.8360998034477234,
        0.8434733152389526,
        0.843326985836029,
        0.8308660984039307,
        0.8370453715324402,
        0.8486795425415039,
        0.8464033603668213,
        0.8394812345504761,
        0.8352814316749573,
        0.8494672179222107,
        0.8333716988563538,
        0.8404858112335205,
        0.8402252793312073,
        0.82843416929245,
        0.8357222080230713,
        0.8396517634391785,
        0.8378745913505554,
        0.8366497755050659,
        0.8347772359848022,
        0.8350527286529541,
        0.8383533358573914,
        0.8473098874092102,
        0.8397688269615173,
        0.8323898315429688,
        0.8399345278739929,
        0.835133969783783,
        0.8472349643707275
    ],
    "lr": [
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.0010000000474974513,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        0.00020000000949949026,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05,
        9.999999747378752e-05
    ],
    "number_of_gpus": 1,
    "hostname": "wsdmal",
    "training_time": "0:14:43:13",
    "training_files": [
        "/scratch/dmal/asip/ds_v2/20180314T202722",
        "/scratch/dmal/asip/ds_v2/20180314T202827",
        "/scratch/dmal/asip/ds_v2/20180315T184223",
        "/scratch/dmal/asip/ds_v2/20180319T194714",
        "/scratch/dmal/asip/ds_v2/20180319T203534",
        "/scratch/dmal/asip/ds_v2/20180320T202619",
        "/scratch/dmal/asip/ds_v2/20180320T202723",
        "/scratch/dmal/asip/ds_v2/20180322T080325",
        "/scratch/dmal/asip/ds_v2/20180322T080425",
        "/scratch/dmal/asip/ds_v2/20180322T080525",
        "/scratch/dmal/asip/ds_v2/20180323T093636",
        "/scratch/dmal/asip/ds_v2/20180323T093736",
        "/scratch/dmal/asip/ds_v2/20180323T205304",
        "/scratch/dmal/asip/ds_v2/20180325T194759",
        "/scratch/dmal/asip/ds_v2/20180326T202723",
        "/scratch/dmal/asip/ds_v2/20180326T202827",
        "/scratch/dmal/asip/ds_v2/20180327T103959",
        "/scratch/dmal/asip/ds_v2/20180327T201805",
        "/scratch/dmal/asip/ds_v2/20180327T201909",
        "/scratch/dmal/asip/ds_v2/20180329T200343",
        "/scratch/dmal/asip/ds_v2/20180329T205212",
        "/scratch/dmal/asip/ds_v2/20180331T203638",
        "/scratch/dmal/asip/ds_v2/20180401T202620",
        "/scratch/dmal/asip/ds_v2/20180401T202724",
        "/scratch/dmal/asip/ds_v2/20180404T093636",
        "/scratch/dmal/asip/ds_v2/20180404T205405",
        "/scratch/dmal/asip/ds_v2/20180404T205505",
        "/scratch/dmal/asip/ds_v2/20180406T203739",
        "/scratch/dmal/asip/ds_v2/20180406T203839",
        "/scratch/dmal/asip/ds_v2/20180407T202723",
        "/scratch/dmal/asip/ds_v2/20180407T202827",
        "/scratch/dmal/asip/ds_v2/20180410T084537",
        "/scratch/dmal/asip/ds_v2/20180410T084637",
        "/scratch/dmal/asip/ds_v2/20180413T202620",
        "/scratch/dmal/asip/ds_v2/20180413T202724",
        "/scratch/dmal/asip/ds_v2/20180416T093737",
        "/scratch/dmal/asip/ds_v2/20180416T205100",
        "/scratch/dmal/asip/ds_v2/20180416T205305",
        "/scratch/dmal/asip/ds_v2/20180416T205405",
        "/scratch/dmal/asip/ds_v2/20180417T195529",
        "/scratch/dmal/asip/ds_v2/20180418T203434",
        "/scratch/dmal/asip/ds_v2/20180418T203739",
        "/scratch/dmal/asip/ds_v2/20180418T203839",
        "/scratch/dmal/asip/ds_v2/20180420T201806",
        "/scratch/dmal/asip/ds_v2/20180421T080246",
        "/scratch/dmal/asip/ds_v2/20180421T080346",
        "/scratch/dmal/asip/ds_v2/20180421T080446",
        "/scratch/dmal/asip/ds_v2/20180421T201241",
        "/scratch/dmal/asip/ds_v2/20180423T195615",
        "/scratch/dmal/asip/ds_v2/20180424T194715",
        "/scratch/dmal/asip/ds_v2/20180424T203535",
        "/scratch/dmal/asip/ds_v2/20180424T203639",
        "/scratch/dmal/asip/ds_v2/20180426T201915",
        "/scratch/dmal/asip/ds_v2/20180426T202020",
        "/scratch/dmal/asip/ds_v2/20180427T080327",
        "/scratch/dmal/asip/ds_v2/20180427T080427",
        "/scratch/dmal/asip/ds_v2/20180427T080527",
        "/scratch/dmal/asip/ds_v2/20180427T103155",
        "/scratch/dmal/asip/ds_v2/20180428T093637",
        "/scratch/dmal/asip/ds_v2/20180428T093737",
        "/scratch/dmal/asip/ds_v2/20180506T203536",
        "/scratch/dmal/asip/ds_v2/20180506T203640",
        "/scratch/dmal/asip/ds_v2/20180507T202621",
        "/scratch/dmal/asip/ds_v2/20180507T202725",
        "/scratch/dmal/asip/ds_v2/20180508T121701",
        "/scratch/dmal/asip/ds_v2/20180508T121806",
        "/scratch/dmal/asip/ds_v2/20180510T200427",
        "/scratch/dmal/asip/ds_v2/20180512T203436",
        "/scratch/dmal/asip/ds_v2/20180512T203540",
        "/scratch/dmal/asip/ds_v2/20180515T080247",
        "/scratch/dmal/asip/ds_v2/20180515T080347",
        "/scratch/dmal/asip/ds_v2/20180515T080447",
        "/scratch/dmal/asip/ds_v2/20180517T101624",
        "/scratch/dmal/asip/ds_v2/20180517T101724",
        "/scratch/dmal/asip/ds_v2/20180518T203536",
        "/scratch/dmal/asip/ds_v2/20180518T203640",
        "/scratch/dmal/asip/ds_v2/20180521T103052",
        "/scratch/dmal/asip/ds_v2/20180521T103156",
        "/scratch/dmal/asip/ds_v2/20180521T183450",
        "/scratch/dmal/asip/ds_v2/20180521T183554",
        "/scratch/dmal/asip/ds_v2/20180522T205407",
        "/scratch/dmal/asip/ds_v2/20180530T203641",
        "/scratch/dmal/asip/ds_v2/20180531T203027",
        "/scratch/dmal/asip/ds_v2/20180603T205103",
        "/scratch/dmal/asip/ds_v2/20180603T205207",
        "/scratch/dmal/asip/ds_v2/20180604T195532",
        "/scratch/dmal/asip/ds_v2/20180607T103858",
        "/scratch/dmal/asip/ds_v2/20180607T104003",
        "/scratch/dmal/asip/ds_v2/20180607T184226",
        "/scratch/dmal/asip/ds_v2/20180607T184326",
        "/scratch/dmal/asip/ds_v2/20180610T101626",
        "/scratch/dmal/asip/ds_v2/20180610T195617",
        "/scratch/dmal/asip/ds_v2/20180611T203538",
        "/scratch/dmal/asip/ds_v2/20180611T203642",
        "/scratch/dmal/asip/ds_v2/20180612T185212",
        "/scratch/dmal/asip/ds_v2/20180613T121704",
        "/scratch/dmal/asip/ds_v2/20180613T121808",
        "/scratch/dmal/asip/ds_v2/20180617T203438",
        "/scratch/dmal/asip/ds_v2/20180617T203542",
        "/scratch/dmal/asip/ds_v2/20180617T203742",
        "/scratch/dmal/asip/ds_v2/20180617T203842",
        "/scratch/dmal/asip/ds_v2/20180617T203942",
        "/scratch/dmal/asip/ds_v2/20180617T204042",
        "/scratch/dmal/asip/ds_v2/20180619T184227",
        "/scratch/dmal/asip/ds_v2/20180619T184327",
        "/scratch/dmal/asip/ds_v2/20180620T120957",
        "/scratch/dmal/asip/ds_v2/20180623T203538",
        "/scratch/dmal/asip/ds_v2/20180624T202624",
        "/scratch/dmal/asip/ds_v2/20180624T202728",
        "/scratch/dmal/asip/ds_v2/20180625T211138",
        "/scratch/dmal/asip/ds_v2/20180625T211238",
        "/scratch/dmal/asip/ds_v2/20180626T183556",
        "/scratch/dmal/asip/ds_v2/20180627T120051",
        "/scratch/dmal/asip/ds_v2/20180627T205105",
        "/scratch/dmal/asip/ds_v2/20180627T205209",
        "/scratch/dmal/asip/ds_v2/20180627T205509",
        "/scratch/dmal/asip/ds_v2/20180629T203439",
        "/scratch/dmal/asip/ds_v2/20180629T203543",
        "/scratch/dmal/asip/ds_v2/20180629T203743",
        "/scratch/dmal/asip/ds_v2/20180701T184328",
        "/scratch/dmal/asip/ds_v2/20180701T201810",
        "/scratch/dmal/asip/ds_v2/20180702T210335",
        "/scratch/dmal/asip/ds_v2/20180702T210435",
        "/scratch/dmal/asip/ds_v2/20180703T124944",
        "/scratch/dmal/asip/ds_v2/20180703T200348",
        "/scratch/dmal/asip/ds_v2/20180704T110035",
        "/scratch/dmal/asip/ds_v2/20180705T123432",
        "/scratch/dmal/asip/ds_v2/20180706T104718",
        "/scratch/dmal/asip/ds_v2/20180706T104818",
        "/scratch/dmal/asip/ds_v2/20180709T200430",
        "/scratch/dmal/asip/ds_v2/20180711T203439",
        "/scratch/dmal/asip/ds_v2/20180713T121621",
        "/scratch/dmal/asip/ds_v2/20180715T205322",
        "/scratch/dmal/asip/ds_v2/20180718T081954",
        "/scratch/dmal/asip/ds_v2/20180718T082054",
        "/scratch/dmal/asip/ds_v2/20180718T202625",
        "/scratch/dmal/asip/ds_v2/20180720T120811",
        "/scratch/dmal/asip/ds_v2/20180724T082011",
        "/scratch/dmal/asip/ds_v2/20180724T211956",
        "/scratch/dmal/asip/ds_v2/20180727T205219",
        "/scratch/dmal/asip/ds_v2/20180727T205323",
        "/scratch/dmal/asip/ds_v2/20180728T110419",
        "/scratch/dmal/asip/ds_v2/20180728T195620",
        "/scratch/dmal/asip/ds_v2/20180730T202626",
        "/scratch/dmal/asip/ds_v2/20180731T121706",
        "/scratch/dmal/asip/ds_v2/20180802T093643",
        "/scratch/dmal/asip/ds_v2/20180803T195536",
        "/scratch/dmal/asip/ds_v2/20180804T114430",
        "/scratch/dmal/asip/ds_v2/20180808T075536",
        "/scratch/dmal/asip/ds_v2/20180808T075636",
        "/scratch/dmal/asip/ds_v2/20180811T081955",
        "/scratch/dmal/asip/ds_v2/20180811T082055",
        "/scratch/dmal/asip/ds_v2/20180811T202627",
        "/scratch/dmal/asip/ds_v2/20180812T121811",
        "/scratch/dmal/asip/ds_v2/20180814T200433",
        "/scratch/dmal/asip/ds_v2/20180814T205107",
        "/scratch/dmal/asip/ds_v2/20180815T195536",
        "/scratch/dmal/asip/ds_v2/20180817T193910",
        "/scratch/dmal/asip/ds_v2/20180818T081106",
        "/scratch/dmal/asip/ds_v2/20180818T081206",
        "/scratch/dmal/asip/ds_v2/20180818T081306",
        "/scratch/dmal/asip/ds_v2/20180818T085755",
        "/scratch/dmal/asip/ds_v2/20180818T085855",
        "/scratch/dmal/asip/ds_v2/20180820T101940",
        "/scratch/dmal/asip/ds_v2/20180822T082424",
        "/scratch/dmal/asip/ds_v2/20180822T082524",
        "/scratch/dmal/asip/ds_v2/20180822T082624",
        "/scratch/dmal/asip/ds_v2/20180824T081102",
        "/scratch/dmal/asip/ds_v2/20180824T081202",
        "/scratch/dmal/asip/ds_v2/20180824T094625",
        "/scratch/dmal/asip/ds_v2/20180824T094725",
        "/scratch/dmal/asip/ds_v2/20180901T205221",
        "/scratch/dmal/asip/ds_v2/20180902T115237",
        "/scratch/dmal/asip/ds_v2/20180902T115341",
        "/scratch/dmal/asip/ds_v2/20180903T082525",
        "/scratch/dmal/asip/ds_v2/20180903T082725",
        "/scratch/dmal/asip/ds_v2/20180906T080234",
        "/scratch/dmal/asip/ds_v2/20180909T100819",
        "/scratch/dmal/asip/ds_v2/20180910T081814",
        "/scratch/dmal/asip/ds_v2/20180910T081914",
        "/scratch/dmal/asip/ds_v2/20180914T074628",
        "/scratch/dmal/asip/ds_v2/20180915T082725",
        "/scratch/dmal/asip/ds_v2/20180915T082825",
        "/scratch/dmal/asip/ds_v2/20180915T203543",
        "/scratch/dmal/asip/ds_v2/20180918T103203",
        "/scratch/dmal/asip/ds_v2/20180921T194808",
        "/scratch/dmal/asip/ds_v2/20180923T201918",
        "/scratch/dmal/asip/ds_v2/20180924T080254",
        "/scratch/dmal/asip/ds_v2/20180924T210235",
        "/scratch/dmal/asip/ds_v2/20180925T200352",
        "/scratch/dmal/asip/ds_v2/20180929T081203",
        "/scratch/dmal/asip/ds_v2/20181001T102424",
        "/scratch/dmal/asip/ds_v2/20181001T205214",
        "/scratch/dmal/asip/ds_v2/20181002T190816",
        "/scratch/dmal/asip/ds_v2/20181002T195538",
        "/scratch/dmal/asip/ds_v2/20181003T203548",
        "/scratch/dmal/asip/ds_v2/20181004T180321",
        "/scratch/dmal/asip/ds_v2/20181009T181232",
        "/scratch/dmal/asip/ds_v2/20181010T202629",
        "/scratch/dmal/asip/ds_v2/20181013T205110",
        "/scratch/dmal/asip/ds_v2/20181021T203544",
        "/scratch/dmal/asip/ds_v2/20181022T082058",
        "/scratch/dmal/asip/ds_v2/20181022T180329",
        "/scratch/dmal/asip/ds_v2/20181022T180429",
        "/scratch/dmal/asip/ds_v2/20181022T180529",
        "/scratch/dmal/asip/ds_v2/20181023T121709",
        "/scratch/dmal/asip/ds_v2/20181024T201208",
        "/scratch/dmal/asip/ds_v2/20181028T193912",
        "/scratch/dmal/asip/ds_v2/20181029T201919",
        "/scratch/dmal/asip/ds_v2/20181030T080255",
        "/scratch/dmal/asip/ds_v2/20181030T080355",
        "/scratch/dmal/asip/ds_v2/20181030T080455",
        "/scratch/dmal/asip/ds_v2/20181101T101632",
        "/scratch/dmal/asip/ds_v2/20181101T115238",
        "/scratch/dmal/asip/ds_v2/20181101T115342",
        "/scratch/dmal/asip/ds_v2/20181102T203543",
        "/scratch/dmal/asip/ds_v2/20181102T203648",
        "/scratch/dmal/asip/ds_v2/20181103T202733",
        "/scratch/dmal/asip/ds_v2/20181104T121709",
        "/scratch/dmal/asip/ds_v2/20181104T121813",
        "/scratch/dmal/asip/ds_v2/20181105T103203",
        "/scratch/dmal/asip/ds_v2/20181105T183457",
        "/scratch/dmal/asip/ds_v2/20181105T201207",
        "/scratch/dmal/asip/ds_v2/20181106T205214",
        "/scratch/dmal/asip/ds_v2/20181108T203747",
        "/scratch/dmal/asip/ds_v2/20181109T104755",
        "/scratch/dmal/asip/ds_v2/20181109T202836",
        "/scratch/dmal/asip/ds_v2/20181110T081208",
        "/scratch/dmal/asip/ds_v2/20181110T081308",
        "/scratch/dmal/asip/ds_v2/20181114T194724",
        "/scratch/dmal/asip/ds_v2/20181114T203543",
        "/scratch/dmal/asip/ds_v2/20181115T202629",
        "/scratch/dmal/asip/ds_v2/20181116T103946",
        "/scratch/dmal/asip/ds_v2/20181116T104050",
        "/scratch/dmal/asip/ds_v2/20181118T102424",
        "/scratch/dmal/asip/ds_v2/20181120T203443",
        "/scratch/dmal/asip/ds_v2/20181120T203547",
        "/scratch/dmal/asip/ds_v2/20181121T193912",
        "/scratch/dmal/asip/ds_v2/20181122T104008",
        "/scratch/dmal/asip/ds_v2/20181122T201918",
        "/scratch/dmal/asip/ds_v2/20181123T210339",
        "/scratch/dmal/asip/ds_v2/20181124T205221",
        "/scratch/dmal/asip/ds_v2/20181124T205325",
        "/scratch/dmal/asip/ds_v2/20181125T101631",
        "/scratch/dmal/asip/ds_v2/20181126T203647",
        "/scratch/dmal/asip/ds_v2/20181128T103945",
        "/scratch/dmal/asip/ds_v2/20181128T201923",
        "/scratch/dmal/asip/ds_v2/20181128T202027",
        "/scratch/dmal/asip/ds_v2/20181130T205513",
        "/scratch/dmal/asip/ds_v2/20181202T100819",
        "/scratch/dmal/asip/ds_v2/20181202T185924",
        "/scratch/dmal/asip/ds_v2/20181205T201044",
        "/scratch/dmal/asip/ds_v2/20181205T201248",
        "/scratch/dmal/asip/ds_v2/20181206T124948",
        "/scratch/dmal/asip/ds_v2/20181208T203542",
        "/scratch/dmal/asip/ds_v2/20181208T203647",
        "/scratch/dmal/asip/ds_v2/20181210T103945",
        "/scratch/dmal/asip/ds_v2/20181210T104049",
        "/scratch/dmal/asip/ds_v2/20181212T102423",
        "/scratch/dmal/asip/ds_v2/20181212T205512",
        "/scratch/dmal/asip/ds_v2/20181217T210310",
        "/scratch/dmal/asip/ds_v2/20181217T210415",
        "/scratch/dmal/asip/ds_v2/20181218T205325",
        "/scratch/dmal/asip/ds_v2/20181219T101630",
        "/scratch/dmal/asip/ds_v2/20181227T081913",
        "/scratch/dmal/asip/ds_v2/20181227T082013",
        "/scratch/dmal/asip/ds_v2/20181227T082113",
        "/scratch/dmal/asip/ds_v2/20190104T103202",
        "/scratch/dmal/asip/ds_v2/20190105T205535",
        "/scratch/dmal/asip/ds_v2/20190110T183635",
        "/scratch/dmal/asip/ds_v2/20190124T101555",
        "/scratch/dmal/asip/ds_v2/20190124T101659",
        "/scratch/dmal/asip/ds_v2/20190201T082112",
        "/scratch/dmal/asip/ds_v2/20190208T081100",
        "/scratch/dmal/asip/ds_v2/20190208T081200",
        "/scratch/dmal/asip/ds_v2/20190208T081300",
        "/scratch/dmal/asip/ds_v2/20190208T201920",
        "/scratch/dmal/asip/ds_v2/20190208T202025",
        "/scratch/dmal/asip/ds_v2/20190210T120052",
        "/scratch/dmal/asip/ds_v2/20190210T120156",
        "/scratch/dmal/asip/ds_v2/20190210T205411",
        "/scratch/dmal/asip/ds_v2/20190213T202729",
        "/scratch/dmal/asip/ds_v2/20190214T090209",
        "/scratch/dmal/asip/ds_v2/20190216T205323",
        "/scratch/dmal/asip/ds_v2/20190216T205423",
        "/scratch/dmal/asip/ds_v2/20190218T203540",
        "/scratch/dmal/asip/ds_v2/20190222T205511",
        "/scratch/dmal/asip/ds_v2/20190223T195535",
        "/scratch/dmal/asip/ds_v2/20190224T100817",
        "/scratch/dmal/asip/ds_v2/20190224T100917",
        "/scratch/dmal/asip/ds_v2/20190226T090313",
        "/scratch/dmal/asip/ds_v2/20190226T201916",
        "/scratch/dmal/asip/ds_v2/20190226T211043",
        "/scratch/dmal/asip/ds_v2/20190228T205218",
        "/scratch/dmal/asip/ds_v2/20190228T205323",
        "/scratch/dmal/asip/ds_v2/20190301T101628",
        "/scratch/dmal/asip/ds_v2/20190301T101728",
        "/scratch/dmal/asip/ds_v2/20190302T203540",
        "/scratch/dmal/asip/ds_v2/20190302T203644",
        "/scratch/dmal/asip/ds_v2/20190304T103943",
        "/scratch/dmal/asip/ds_v2/20190312T205423",
        "/scratch/dmal/asip/ds_v2/20190314T203540",
        "/scratch/dmal/asip/ds_v2/20190314T203644",
        "/scratch/dmal/asip/ds_v2/20190318T102525",
        "/scratch/dmal/asip/ds_v2/20190320T100817",
        "/scratch/dmal/asip/ds_v2/20190320T100917",
        "/scratch/dmal/asip/ds_v2/20190322T104105",
        "/scratch/dmal/asip/ds_v2/20190324T205323",
        "/scratch/dmal/asip/ds_v2/20190325T101628",
        "/scratch/dmal/asip/ds_v2/20190325T101728",
        "/scratch/dmal/asip/ds_v2/20190325T195620",
        "/scratch/dmal/asip/ds_v2/20190326T203540",
        "/scratch/dmal/asip/ds_v2/20190330T102526",
        "/scratch/dmal/asip/ds_v2/20190401T203441",
        "/scratch/dmal/asip/ds_v2/20190404T201042",
        "/scratch/dmal/asip/ds_v2/20190404T201246",
        "/scratch/dmal/asip/ds_v2/20190405T205423",
        "/scratch/dmal/asip/ds_v2/20190407T123329",
        "/scratch/dmal/asip/ds_v2/20190409T103943",
        "/scratch/dmal/asip/ds_v2/20190409T104048",
        "/scratch/dmal/asip/ds_v2/20190411T102421",
        "/scratch/dmal/asip/ds_v2/20190413T100817",
        "/scratch/dmal/asip/ds_v2/20190413T100917",
        "/scratch/dmal/asip/ds_v2/20190415T211043",
        "/scratch/dmal/asip/ds_v2/20190417T205423",
        "/scratch/dmal/asip/ds_v2/20190423T102526",
        "/scratch/dmal/asip/ds_v2/20190423T200433",
        "/scratch/dmal/asip/ds_v2/20190423T205412",
        "/scratch/dmal/asip/ds_v2/20190425T100818",
        "/scratch/dmal/asip/ds_v2/20190427T201813",
        "/scratch/dmal/asip/ds_v2/20190427T201917",
        "/scratch/dmal/asip/ds_v2/20190427T211044",
        "/scratch/dmal/asip/ds_v2/20190429T200351",
        "/scratch/dmal/asip/ds_v2/20190429T205424",
        "/scratch/dmal/asip/ds_v2/20190509T081206",
        "/scratch/dmal/asip/ds_v2/20190517T102423",
        "/scratch/dmal/asip/ds_v2/20190519T194808",
        "/scratch/dmal/asip/ds_v2/20190523T200352",
        "/scratch/dmal/asip/ds_v2/20190524T101631",
        "/scratch/dmal/asip/ds_v2/20190525T203543",
        "/scratch/dmal/asip/ds_v2/20190525T203647"
    ],
    "test_files": [
        "/scratch/dmal/asip/ds_v2/20180315T184323",
        "/scratch/dmal/asip/ds_v2/20180323T205404",
        "/scratch/dmal/asip/ds_v2/20180331T203534",
        "/scratch/dmal/asip/ds_v2/20180404T205305",
        "/scratch/dmal/asip/ds_v2/20180406T203434",
        "/scratch/dmal/asip/ds_v2/20180406T203539",
        "/scratch/dmal/asip/ds_v2/20180409T201140",
        "/scratch/dmal/asip/ds_v2/20180416T093637",
        "/scratch/dmal/asip/ds_v2/20180418T203539",
        "/scratch/dmal/asip/ds_v2/20180420T201910",
        "/scratch/dmal/asip/ds_v2/20180424T100720",
        "/scratch/dmal/asip/ds_v2/20180510T205406",
        "/scratch/dmal/asip/ds_v2/20180521T201200",
        "/scratch/dmal/asip/ds_v2/20180530T203537",
        "/scratch/dmal/asip/ds_v2/20180610T101726",
        "/scratch/dmal/asip/ds_v2/20180612T185108",
        "/scratch/dmal/asip/ds_v2/20180620T120853",
        "/scratch/dmal/asip/ds_v2/20180620T210334",
        "/scratch/dmal/asip/ds_v2/20180626T183452",
        "/scratch/dmal/asip/ds_v2/20180627T120155",
        "/scratch/dmal/asip/ds_v2/20180627T205409",
        "/scratch/dmal/asip/ds_v2/20180627T205609",
        "/scratch/dmal/asip/ds_v2/20180629T203843",
        "/scratch/dmal/asip/ds_v2/20180701T184228",
        "/scratch/dmal/asip/ds_v2/20180701T201914",
        "/scratch/dmal/asip/ds_v2/20180717T203540",
        "/scratch/dmal/asip/ds_v2/20180722T195535",
        "/scratch/dmal/asip/ds_v2/20180728T110315",
        "/scratch/dmal/asip/ds_v2/20180730T180326",
        "/scratch/dmal/asip/ds_v2/20180804T114534",
        "/scratch/dmal/asip/ds_v2/20180806T184330",
        "/scratch/dmal/asip/ds_v2/20180809T195621",
        "/scratch/dmal/asip/ds_v2/20180810T203541",
        "/scratch/dmal/asip/ds_v2/20180812T121707",
        "/scratch/dmal/asip/ds_v2/20180817T095536",
        "/scratch/dmal/asip/ds_v2/20180826T093044",
        "/scratch/dmal/asip/ds_v2/20180903T082625",
        "/scratch/dmal/asip/ds_v2/20180906T080334",
        "/scratch/dmal/asip/ds_v2/20180914T074728",
        "/scratch/dmal/asip/ds_v2/20180921T100820",
        "/scratch/dmal/asip/ds_v2/20180923T201814",
        "/scratch/dmal/asip/ds_v2/20180929T081103",
        "/scratch/dmal/asip/ds_v2/20181004T180221",
        "/scratch/dmal/asip/ds_v2/20181005T112905",
        "/scratch/dmal/asip/ds_v2/20181015T203848",
        "/scratch/dmal/asip/ds_v2/20181015T203948",
        "/scratch/dmal/asip/ds_v2/20181021T203648",
        "/scratch/dmal/asip/ds_v2/20181023T121814",
        "/scratch/dmal/asip/ds_v2/20181029T201815",
        "/scratch/dmal/asip/ds_v2/20181105T183601",
        "/scratch/dmal/asip/ds_v2/20181108T203443",
        "/scratch/dmal/asip/ds_v2/20181109T104859",
        "/scratch/dmal/asip/ds_v2/20181109T202732",
        "/scratch/dmal/asip/ds_v2/20181114T203647",
        "/scratch/dmal/asip/ds_v2/20181121T202732",
        "/scratch/dmal/asip/ds_v2/20181121T202836",
        "/scratch/dmal/asip/ds_v2/20181128T104050",
        "/scratch/dmal/asip/ds_v2/20181129T103203",
        "/scratch/dmal/asip/ds_v2/20181130T205613",
        "/scratch/dmal/asip/ds_v2/20181205T201148",
        "/scratch/dmal/asip/ds_v2/20181211T103058",
        "/scratch/dmal/asip/ds_v2/20181214T203747",
        "/scratch/dmal/asip/ds_v2/20181218T205220",
        "/scratch/dmal/asip/ds_v2/20190110T183531",
        "/scratch/dmal/asip/ds_v2/20190112T101700",
        "/scratch/dmal/asip/ds_v2/20190206T203540",
        "/scratch/dmal/asip/ds_v2/20190206T203644",
        "/scratch/dmal/asip/ds_v2/20190213T202833",
        "/scratch/dmal/asip/ds_v2/20190214T090313",
        "/scratch/dmal/asip/ds_v2/20190216T205218",
        "/scratch/dmal/asip/ds_v2/20190218T203644",
        "/scratch/dmal/asip/ds_v2/20190225T202729",
        "/scratch/dmal/asip/ds_v2/20190225T202833",
        "/scratch/dmal/asip/ds_v2/20190226T090209",
        "/scratch/dmal/asip/ds_v2/20190226T201811",
        "/scratch/dmal/asip/ds_v2/20190304T104047",
        "/scratch/dmal/asip/ds_v2/20190310T211043",
        "/scratch/dmal/asip/ds_v2/20190324T205219",
        "/scratch/dmal/asip/ds_v2/20190330T205411",
        "/scratch/dmal/asip/ds_v2/20190404T201146",
        "/scratch/dmal/asip/ds_v2/20190405T084543",
        "/scratch/dmal/asip/ds_v2/20190411T205411",
        "/scratch/dmal/asip/ds_v2/20190419T203541",
        "/scratch/dmal/asip/ds_v2/20190419T203645",
        "/scratch/dmal/asip/ds_v2/20190509T081306"
    ],
    "modelname": "model-ds2-500",
    "modeltype": "<function ASPP_model_extdata at 0x7f2257ec3a60>",
    "batchsize": 8,
    "epochs": 60,
    "period_weights": 10,
    "seed": 18374,
    "ice_threshold": null,
    "extdata": [],
    "path": "/scratch/dmal/asip/ds_v2/file_list.json",
    "train_code": "    #!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Jun 20 19:15:41 2017\n\n@author: dmal\n\"\"\"\n\nimport os, json\nimport numpy as np\nimport pandas as pd\nfrom asiplib import sliding_window\nfrom scipy.special import logit\n\nfrom generators import generators\nfrom generator_v2 import asip_generator\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, ReduceLROnPlateau\nfrom keras.utils import multi_gpu_model\nfrom keras.models import model_from_json\nfrom models import *\nfrom tensorflow.python.client import device_lib\n\ndef jsonize(obj):\n    if isinstance(obj, dict):\n        for k, v in obj.items():\n            obj[jsonize(k)] = jsonize(v)\n        return obj\n    elif isinstance(obj, (np.ndarray, list, tuple)):\n        return [jsonize(i) for i in obj]\n    elif type(obj) in (str, int, float, bool, type(None)):\n        return obj\n    elif callable(obj):\n        return str(obj)\n    elif hasattr(obj, 'item'):\n        # np.int32, np.uint8, np.float32, np.float64, ...\n        return obj.item()\n    else:\n        print(\"--- UNKNOWN TYPE %s ---\" % type(obj))\n        raise Exception('unserializable: %s' % type(obj))\n    return obj\n\ndef logit_(y):\n    y[y==0] += 0.0001\n    y[y==1] -= 0.0001\n    return(logit(y))\n\ndef get_available_gpus():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n\n### Parameters ###\nparam = {\"modelname\" : \"model-aspp-mse\", \n         \"modeltype\" : ASPP_model_extdata,\n         \"batchsize\" : 64,\n         \"epochs\" : 80,\n         \"period_weights\" : 10,\n         \"seed\" : 18374,\n         \"ice_threshold\" : None,\n         \"extdata\" : ['amsr'], #['inc','amsr','time','location']\n         \"path\" : \"/nobackup/dmal/asip/ds4/\",\n         \"train_code\" : open(\"train_model.py\", \"r\").read(),\n         \"model_code\" : open(\"models.py\", \"r\").read(),\n         \"generator_code\" : open(\"generators.py\", \"r\").read(),\n         \"amsr_file\" : \"amsr_coeffs.csv\",\n         \"single_output\" : False,\n         \"y_scale\" : 100,\n         \"link_func\" : None,#logit_,\n         \"loss_func\" : \"mean_squared_error\",\n         \"metrics\" : ['mse'],\n         \"optimizer\" : \"Adam\"}\nnp.random.seed(param['seed'])\n\nif param['amsr_file']:\n    df = pd.read_csv(param['amsr_file'])\n    means, stdvs = (df['means'].values, np.sqrt(df['variances'].values))\n\nfiles = [elem for elem in os.listdir(param['path']) if elem.endswith(\".npz\")]\ntest_files = [param['path']+files[k]\n            for k in np.random.randint(0, len(files), int(0.1*len(files)))]\ntrain_files = [param['path']+elem for elem in files if elem not in test_files]\n\n# files = np.unique([param['path']+elem.split(\"_\")[0] for elem in os.listdir(param['path']) if elem.endswith(\".npy\")])\n# np.random.shuffle(files)\n# test_files = files[:int(0.1*len(files))]\n# train_files = files[int(0.1*len(files)):]\n\ngens = generators(train_files=train_files,\n                  test_files=test_files,\n                  icethreshold=param['ice_threshold'], \n                  extdata=param['extdata'],\n                  batch_size=param['batchsize'],\n                  amsr_norm=(means,stdvs),\n                  single_output=param['single_output'],\n                  link_func=param[\"link_func\"],\n                  y_scale=param[\"y_scale\"])\nnb_trainsamples = gens.nb_trainsamples\nnb_testsamples = gens.nb_testsamples\ntrain_generator = gens.train_gen_ext\ntest_generator = gens.test_gen_ext\nbatch = next(test_generator())\n\n# train_generator = asip_generator(train_files, None, param[\"extdata\"], param[\"batchsize\"], single_output=param[\"single_output\"], amsr_norm=[means, stdvs], link_func=param['link'])\n# test_generator = asip_generator(test_files, None, param[\"extdata\"], param[\"batchsize\"], single_output=param[\"single_output\"], amsr_norm=[means, stdvs], train_mode=False, link_func=param['link'])\n# nb_trainsamples = train_generator.nb_samples\n# nb_testsamples = test_generator.nb_samples\n# batch = next(test_generator.data_generation())\n\nif param['loss_func'] == \"mean_squared_error\":\n    act = None\nelse:\n    act = 'sigmoid'\n\nmodel = param['modeltype']([elem.shape[1:] for elem in batch[0]], output_act=act)\ndel(batch)\n\n\nif os.path.isfile(param['modelname']+\"_config.json\") and os.path.isfile(param['modelname']+\".h5\"):\n    if os.path.isfile(param['modelname']+\"_history.json\"):\n        json_hist = json.load(open(param['modelname']+\"_history.json\",\"r\"))\n    else:\n        json_hist = {'val_acc':[],'acc':[],'loss':[],'val_loss':[]}\n\n    json_str = json.load(open(param['modelname']+\"_config.json\",\"r\"))\n    model = model_from_json(json_str)\n    model.load_weights(param['modelname']+\".h5\")\n    log = np.genfromtxt(param['modelname']+\".log\", delimiter=\",\",skip_header=1)\n    e_init = int(log[-1,0] // param['period_weights'] * param['period_weights'])\n    model.compile(loss=param['loss_func'], optimizer=param['optimizer'], metrics=param['metrics'])\n    print(\"Continuing training process from epoch %d\" % e_init)\nelse:\n    model.compile(loss=param['loss_func'], optimizer=param['optimizer'], metrics=param['metrics'])\n    json_string = model.to_json()\n    json.dump(json_string, open(param['modelname']+\"_config.json\", \"w\"))\n    model_object = model\n    e_init = 0\n    json_hist = {'loss':[],'val_loss':[]}\n    for elem in param['metrics']:\n        json_hist['val_'+elem] =[]\n        json_hist[elem] =[]\n\n#####################################\n# Train Model                       #\n#####################################\n\nnb_gpus = len(get_available_gpus())\nif nb_gpus > 1:\n    m_model = multi_gpu_model(model, gpus=nb_gpus)\n    m_model.compile(loss=param['loss_func'], optimizer=param['optimizer'], metrics=param['metrics'])\n    model_object = m_model\n\n\nimport time\nstart = time.time()\nhistory = model_object.fit_generator(train_generator(),\n                                     steps_per_epoch=np.ceil(nb_trainsamples//param['batchsize']).astype(int),\n                                     callbacks=[ModelCheckpoint(\"tmp/\"+param['modelname'].split('/')[-1]+\".{epoch:02d}.h5\",\n                                                                monitor='loss',\n                                                                verbose=1,\n                                                                period=param['period_weights'],\n                                                                save_weights_only=True),\n                                                CSVLogger(param['modelname']+'.log'),\n                                                ReduceLROnPlateau(monitor='loss', \n                                                                  factor=0.2,\n                                                                  patience=5, \n                                                                  min_lr=0.0001,\n                                                                  min_delta=0.001)],\n                                     validation_data=test_generator(), \n                                     validation_steps=np.ceil(nb_testsamples//param['batchsize']).astype(int),\n                                     epochs=param['epochs'], \n                                     max_queue_size=10, \n                                     verbose=1, \n                                     initial_epoch=e_init)\nseconds = time.time()-start\nm, s = divmod(seconds, 60)\nh, m = divmod(m, 60)\nd, h = divmod(h, 24)\nprint('It took %d:%d:%02d:%02d to train.' % (d, h, m, s))\n\nmodel.save_weights(param['modelname']+\".h5\")\nimport json\nimport socket\ndct = history.history\n\nfor elem in ['acc', 'mse', 'mean_squared_error']:\n    if (elem in json_hist.keys()) and (elem in dct.keys()):\n        dct[elem] = json_hist[elem]+dct[elem]\n        dct['val_'+elem] = json_hist['val_'+elem]+dct['val_'+elem]\n\ndct['loss'] = json_hist['loss']+dct['loss']\ndct['val_loss'] = json_hist['val_loss']+dct['val_loss']\n\ndct[\"number_of_gpus\"] = nb_gpus\ndct[\"hostname\"] = socket.gethostname()\ndct[\"training_time\"] = '%d:%d:%02d:%02d' % (d, h, m, s)\ndct[\"training_files\"] = str(train_files)\ndct[\"test_files\"] = str(test_files)\ndct.update(param)\n\njson.dump(jsonize(dct), open(param['modelname']+\"_history.json\", 'w'))\n",
    "model_code": "# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom keras.layers import Conv2D, Conv3D, Activation, Input, Concatenate\nfrom keras.layers import Reshape, AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.layers import UpSampling2D, Flatten, Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model, Sequential\nimport numpy as np\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\n\ndef gkern(shape, dtype=None):\n    \"\"\"\n    creates gaussian kernel with side length l and a sigma of sig\n    \"\"\"\n    assert len(shape)>3\n    \n    if len(shape)==4:\n        h, w, d, o = shape\n    else:\n        h, w, d, v, o = shape\n    \n    sig = np.max((h,w)) // 2\n    xx, yy = np.meshgrid(np.arange(-h // 2 + 1., h // 2 + 1.).astype(dtype), \n                         np.arange(-w // 2 + 1., w // 2 + 1.).astype(dtype))\n\n    kernel = np.exp(-(xx**2 + yy**2) / (2. * sig**2))\n    kernel = np.repeat(np.expand_dims(kernel / np.sum(kernel), -1), d, axis=-1)\n    return(kernel.reshape(shape))\n\ndef ASPP_interpretability_small(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    x = Conv2D(12, (3, 3), padding='same', activation='relu')(inputs[0])\n    x_ = Conv2D(14, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x_)\n    x = Dropout(0.25)(x)\n\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    \n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x1 = AveragePooling2D(pool_size=(2,2), strides=(1,1), padding=\"same\")(x)\n    x2 = AveragePooling2D(pool_size=(4,4), strides=(1,1), padding=\"same\")(x)\n    x3 = AveragePooling2D(pool_size=(8,8), strides=(1,1), padding=\"same\")(x)\n    x4 = AveragePooling2D(pool_size=(16,16), strides=(1,1), padding=\"same\")(x)\n\n    x1 = Conv2D(14, (3, 3), padding='same',\n                  dilation_rate=(2, 2), activation=None)(x1)    \n    x2 = Conv2D(14, (3, 3), padding='same',\n                  dilation_rate=(4, 4), activation=None)(x2)\n    x3 = Conv2D(14, (3, 3), padding='same',\n                  dilation_rate=(8, 8), activation=None)(x3)\n    x4 = Conv2D(14, (3, 3), padding='same',\n                  dilation_rate=(16, 16), activation=None)(x4)\n\n    temp = []\n    for i in range(1,len(insize)):\n        temp.append(UpSampling2D(size=(insize[0][0]//insize[i][0],\n                                     insize[0][1]//insize[i][1]))(inputs[i]))\n\n    x = Concatenate()([x_, x1, x2, x3, x4]+temp)\n    predictions = Conv2D(1, (1, 1), padding='same', activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\ndef ASPP_interpretability_large(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    x = Conv2D(12, (3, 3), padding='same', activation='relu')(inputs[0])\n    x_ = Conv2D(14, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x_)\n    x = Dropout(0.25)(x)\n\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    \n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x1 = AveragePooling2D(pool_size=(2,2), strides=(1,1), padding=\"same\")(x)\n    x2 = AveragePooling2D(pool_size=(4,4), strides=(1,1), padding=\"same\")(x)\n    x3 = AveragePooling2D(pool_size=(8,8), strides=(1,1), padding=\"same\")(x)\n    x4 = AveragePooling2D(pool_size=(16,16), strides=(1,1), padding=\"same\")(x)\n\n    x1 = Conv2D(28, (3, 3), padding='same',\n                  dilation_rate=(2, 2), activation=None)(x1)    \n    x2 = Conv2D(28, (3, 3), padding='same',\n                  dilation_rate=(4, 4), activation=None)(x2)\n    x3 = Conv2D(28, (3, 3), padding='same',\n                  dilation_rate=(8, 8), activation=None)(x3)\n    x4 = Conv2D(28, (3, 3), padding='same',\n                  dilation_rate=(16, 16), activation=None)(x4)\n\n    temp = []\n    for i in range(1,len(insize)):\n        temp.append(UpSampling2D(size=(insize[0][0]//insize[i][0],\n                                     insize[0][1]//insize[i][1]))(inputs[i]))\n\n    x = Concatenate()([x_, x1, x2, x3, x4]+temp)\n    predictions = Conv2D(1, (1, 1), padding='same', activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\n\ndef ASPP_model_extdata(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    x = Conv2D(12, (3, 3), padding='same', activation='relu')(inputs[0])\n    x_ = Conv2D(12, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x_)\n    x = Dropout(0.25)(x)\n\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    \n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x1 = AveragePooling2D(pool_size=(2,2), strides=(1,1), padding=\"same\")(x)\n    x2 = AveragePooling2D(pool_size=(4,4), strides=(1,1), padding=\"same\")(x)\n    x3 = AveragePooling2D(pool_size=(8,8), strides=(1,1), padding=\"same\")(x)\n    x4 = AveragePooling2D(pool_size=(16,16), strides=(1,1), padding=\"same\")(x)\n\n    x1 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(2, 2), activation='relu')(x1)    \n    x2 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(4, 4), activation='relu')(x2)\n    x3 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(8, 8), activation='relu')(x3)\n    x4 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(16, 16), activation='relu')(x4)\n\n    temp = []\n    for i in range(1,len(insize)):\n        temp.append(UpSampling2D(size=(insize[0][0]//insize[i][0],\n                                     insize[0][1]//insize[i][1]))(inputs[i]))\n\n    x = Concatenate()([x_, x1, x2, x3, x4]+temp)\n    predictions = Conv2D(1, (1, 1), padding='same', activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\n\ndef ASPP_model_extdata_v2(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    x = Conv2D(12, (3, 3), padding='same', activation='relu')(inputs[0])\n    x_ = Conv2D(12, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x_)\n    x = Dropout(0.25)(x)\n\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n    \n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n    x = Conv2D(18, (3, 3), padding='same', activation='relu')(x)\n\n    x1 = AveragePooling2D(pool_size=(2,2), strides=(1,1), padding=\"same\")(x)\n    x2 = AveragePooling2D(pool_size=(4,4), strides=(1,1), padding=\"same\")(x)\n    x3 = AveragePooling2D(pool_size=(8,8), strides=(1,1), padding=\"same\")(x)\n    x4 = AveragePooling2D(pool_size=(16,16), strides=(1,1), padding=\"same\")(x)\n\n    x1 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(2, 2), activation='relu')(x1)    \n    x2 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(4, 4), activation='relu')(x2)\n    x3 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(8, 8), activation='relu')(x3)\n    x4 = Conv2D(24, (3, 3), padding='same',\n                  dilation_rate=(16, 16), activation='relu')(x4)\n\n    temp = []\n    if len(insize)>1:\n        for i in range(1,len(insize)):\n            temp.append(UpSampling2D(size=(insize[0][0]//insize[i][0],\n                                        insize[0][1]//insize[i][1]),\n                                        interpolation='bilinear')(inputs[i]))\n\n    x = Concatenate()([x_, x1, x2, x3, x4]+temp)\n    x =  Conv2D(128, (1, 1), padding='same', activation='relu')(x)\n    predictions = Conv2D(1, (1, 1), padding='same', activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\n\n\ndef model_basic(insize, output_act='sigmoid'):\n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3), input_shape=insize,\n                     activation='relu', padding='same'))\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(32, (3, 3), dilation_rate=(2, 2),\n                     activation='relu', padding='same'))\n    model.add(Conv2D(32, (3, 3), dilation_rate=(4, 4), \n                     activation='relu', padding='same'))\n    model.add(Conv2D(32, (3, 3), dilation_rate=(8, 8), \n                     activation='relu', padding='same'))\n    model.add(Conv2D(32, (3, 3), dilation_rate=(1, 1), \n                     activation='relu', padding='same'))\n    model.add(Conv2D(1, (1, 1), activation='linear', \n                     padding='same'))\n    model.add(Activation(output_act))\n\n    return(model)\n\ndef SO_model(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    # inputs = Input(shape=insize)\n\n    x = Conv2D(12, (3, 3), padding='valid', activation=None)(inputs[0])\n    x = Conv2D(12, (3, 3), padding='valid', activation='relu')(x)\n\n    x = MaxPooling2D((3,3))(x)\n    # x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Conv2D(24, (3, 3), padding='valid', activation=None)(x)\n    x = Conv2D(24, (3, 3), padding='valid', activation='relu')(x)\n\n    x = MaxPooling2D((3,3))(x)\n    # x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n\n    x = Conv2D(48, (3, 3), padding='valid', activation=None)(x)\n    x = Conv2D(48, (3, 3), padding='valid', activation='relu')(x)\n\n    # x = MaxPooling2D((3,3))(x)\n    # x = BatchNormalization()(x)\n    # x = Dropout(0.25)(x)\n\n    # x = Conv2D(96, (3, 3), padding='same', activation=None)(x)\n    # x = Conv2D(96, (3, 3), padding='same', activation='relu')(x)\n\n    x = MaxPooling2D((3,3))(x)\n    x = Flatten()(x)\n\n    temp = []\n    if len(insize)>1:\n        for i in range(1,len(insize)):\n            temp.append(Flatten()(inputs[i]))\n        x = Concatenate()([x]+temp)\n        \n    predictions = Dense(1, activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\n\n\ndef anders_model(insize, output_act='sigmoid'):\n    inputs = []\n    for elem in insize:\n        inputs.append(Input(shape=elem))\n\n    # inputs = Input(shape=insize)\n\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(inputs[0])\n    x = MaxPooling2D((2,2))(x) # 300 -> 150\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n    x = MaxPooling2D((2,2))(x) # 148 -> 74\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv2D(64, (3, 3), padding='valid', activation='relu')(x)\n    x = MaxPooling2D((2,2))(x) # 72 -> 36\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = MaxPooling2D((2,2))(x) # 36 -> 18\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n    x = MaxPooling2D((2,2))(x) # 16 -> 8\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv2D(32, (3, 3), padding='valid', activation='relu')(x)\n    x = BatchNormalization()(x) # 6\n    x = Dropout(0.2)(x)\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    \n    if len(insize)>1:\n        temp = []\n        for i in range(1,len(insize)):\n            temp.append(inputs[i])\n        x = Concatenate()([x]+temp)\n        \n    x = BatchNormalization()(x)\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = Dropout(0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dense(1000, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Dense(500, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    x = Dense(250, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    x = Dense(50, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    predictions = Dense(1, activation=output_act)(x)\n\n    model = Model(inputs=inputs, outputs=predictions)\n\n    return(model)\n",
    "generator_code": "import numpy as np\nimport pandas as pd\nimport keras, cv2, json\nfrom osgeo import gdal\n\n    \nclass asip_generator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, file_ids, icethreshold, batch_size, dim=(300,300), return_files=False, \n                 amsr_norm=False, shuffle=True, crops=1, sub_f=1, nersc=False, extdata=False):\n\n        'Initialization'\n        self.ice_threshold = icethreshold\n        self.file_ids = file_ids\n        self.nb_samples = len(file_ids)\n        self.batch_size = batch_size\n        self.files_pr_batch = batch_size // crops\n        self.return_files = return_files\n        self.amsr_norm = amsr_norm\n        self.shuffle = shuffle\n        self.crops = crops\n        self.sub = sub_f\n        self.dim = dim\n        self.extdata = extdata\n\n        if nersc:\n            self.x_channels = [2,3]\n        else:\n            self.x_channels = [0,1]\n\n        fil = np.load(file_ids[0]+\"_x.npy\")\n        h, w, _ = fil.shape\n        self.file_shape = (h, w)        \n        self.ccrop = (h-self.dim[0])//2, (w-self.dim[1])//2 \n        \n        fil = np.load(file_ids[0]+\"_amsr.npy\")\n        self.nb_amsr = fil.shape[2]\n\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return(len(self.file_ids) // self.files_pr_batch)\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.files_pr_batch:(index+1)*self.files_pr_batch]\n\n        # Find list of IDs\n        fbatch = [self.file_ids[k] for k in indexes]\n\n        # Generate data\n        if 'amsr' not in self.extdata:\n            X, y = self.__data_generation_s1(fbatch)\n        else:\n            X, y = self.__data_generation(fbatch)\n\n        if self.return_files:\n            return( X, y, fbatch)\n        else:\n            return( X, y)\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.file_ids))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n\n    def __data_generation(self, files_batch):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, self.dim[0]//self.sub, self.dim[1]//self.sub, len(self.x_channels))) \n        A = np.empty((self.batch_size, self.dim[0]//50, self.dim[1]//50, self.nb_amsr))\n        Y = np.empty((self.batch_size, self.dim[0]//self.sub, self.dim[1]//self.sub, 1))\n\n        # Generate data\n        inds = []\n        for i, fil in enumerate(files_batch):\n            inds_x = np.random.randint(low=0, high=(self.file_shape[0]-self.dim[0])//50, size=self.crops)\n            inds_y = np.random.randint(low=0, high=(self.file_shape[1]-self.dim[1])//50, size=self.crops)\n            for j in range(self.crops):\n                my_dict = json.load(open(\"/\".join(fil.split('/')[:-1])+\"/polygon2sic.json\"))\n                \n                x_ = np.load(fil+\"_x.npy\")[inds_x[j]*50:inds_x[j]*50+self.dim[0],\n                                           inds_y[j]*50:inds_y[j]*50+self.dim[1],\n                                           :2]\n                amsr_ = np.load(fil+\"_amsr.npy\")[inds_x[j]:inds_x[j]+self.dim[0]//50,\n                                           inds_y[j]:inds_y[j]+self.dim[1]//50,\n                                           :]\n                y_ = np.load(fil+\"_y.npy\")[inds_x[j]*50:inds_x[j]*50+self.dim[0],\n                                           inds_y[j]*50:inds_y[j]*50+self.dim[1]]\n                y_ = np.expand_dims(np.vectorize(my_dict.get, otypes=[np.float32])(y_.astype(str)), axis=-1)\n                \n                if self.amsr_norm:\n                    means, stdvs = self.amsr_norm\n                    amsr_ = (amsr_-np.ones((amsr_.shape))*means.reshape((1,1,1,means.size))) / np.reshape(stdvs, (1,1,1,stdvs.size))\n                else:\n                    amsr_ /= 300\n\n                if self.ice_threshold:\n                    y_ = y_ > self.ice_threshold\n\n                if self.sub:\n                        x_ = cv2.resize(x_, None, fx=1/self.sub, fy=1/self.sub, interpolation = cv2.INTER_LINEAR)\n                        y_ = cv2.resize(y_, None, fx=1/self.sub, fy=1/self.sub, interpolation = cv2.INTER_LINEAR)\n\n                X[i+j,], A[i+j,], Y[i+j,] = x_, amsr_, y_\n\n        return([X, A], Y)\n\n    def __data_generation_s1(self, files_batch):\n            'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n            # Initialization\n            X = np.empty((self.batch_size, self.dim[0]//self.sub, self.dim[1]//self.sub, len(self.x_channels))) \n            Y = np.empty((self.batch_size, self.dim[0]//self.sub, self.dim[1]//self.sub, 1))\n\n            # Generate data\n            inds = []\n            for i, fil in enumerate(files_batch):\n                inds_x = np.random.randint(low=0, high=(self.file_shape[0]-self.dim[0])//50, size=self.crops)\n                inds_y = np.random.randint(low=0, high=(self.file_shape[1]-self.dim[1])//50, size=self.crops)\n                for j in range(self.crops):\n                    my_dict = json.load(open(\"/\".join(fil.split('/')[:-1])+\"/polygon2sic.json\"))\n                    \n                    x_ = np.load(fil+\"_x.npy\")[inds_x[j]*50:inds_x[j]*50+self.dim[0],\n                                            inds_y[j]*50:inds_y[j]*50+self.dim[1],\n                                            :2]\n                    y_ = np.load(fil+\"_y.npy\")[inds_x[j]*50:inds_x[j]*50+self.dim[0],\n                                            inds_y[j]*50:inds_y[j]*50+self.dim[1]]\n                    y_ = np.expand_dims(np.vectorize(my_dict.get, otypes=[np.float32])(y_.astype(str)), axis=-1)\n\n                    if self.ice_threshold:\n                        y_ = y_ > self.ice_threshold\n\n                    if self.sub > 1:\n                        x_ = cv2.resize(x_, None, fx=1/self.sub, fy=1/self.sub, interpolation = cv2.INTER_LINEAR)\n                        y_ = np.expand_dims(cv2.resize(y_, None, fx=1/self.sub, fy=1/self.sub, interpolation = cv2.INTER_LINEAR), axis=-1)\n\n                    X[i+j,], Y[i+j,] = x_, y_\n\n            return([X], Y)\n",
    "amsr_file": null,
    "single_output": false,
    "dim": [
        500,
        500
    ],
    "crops": 4,
    "sub_f": 1,
    "nersc": false,
    "y_scale": 100,
    "loss_func": "binary_crossentropy",
    "metrics": [
        "acc"
    ],
    "optimizer": "Adam"
}